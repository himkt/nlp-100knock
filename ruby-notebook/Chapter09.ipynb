{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'countries'\n",
    "require 'numo/narray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80. コーパスの整形\n",
    "\n",
    "文を単語列に変換する最も単純な方法は，空白文字で単語に区切ることである． ただ，この方法では文末のピリオドや括弧などの記号が単語に含まれてしまう． そこで，コーパスの各行のテキストを空白文字でトークンのリストに分割した後，各トークンに以下の処理を施し，単語から記号を除去せよ．\n",
    "\n",
    "トークンの先頭と末尾に出現する次の文字を削除: .,!?;:()[]'\"\n",
    "空文字列となったトークンは削除\n",
    "以上の処理を適用した後，トークンをスペースで連結してファイルに保存せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "doc = []\n",
    "pat = \"[#{[\"\\\\\", \"\\\\.\", \"\\\\,\", \"\\\\!\", \"\\\\?\", \"\\\\;\", \"\\\\:\", \"\\\\(\", \"\\\\)\", \"\\\\[\", \"\\\\]\", \"\\\\'\", \"\\\"\"].join()}]\"\n",
    "\n",
    "File.foreach('../data/enwiki-20150112-400-r100-10576.txt').with_index do |line, index|\n",
    "  p index if index % 50000 == 0\n",
    "  sentence = line.chomp.split.map {|token|\n",
    "    token.\n",
    "      sub(/^#{pat}/, '').\n",
    "      sub(/#{pat}$/, '').\n",
    "      gsub(/#{pat}/, '') # ズル\n",
    "    }.\n",
    "    reject{|token| token.empty?}\n",
    "  \n",
    "  doc << sentence.join(\"\\s\") if sentence.size > 5\n",
    "end\n",
    "\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 81. 複合語からなる国名への対処\n",
    "\n",
    "英語では，複数の語の連接が意味を成すことがある．例えば，アメリカ合衆国は\"United States\"，イギリスは\"United Kingdom\"と表現されるが，\"United\"や\"States\"，\"Kingdom\"という単語だけでは，指し示している概念・実体が曖昧である．そこで，コーパス中に含まれる複合語を認識し，複合語を1語として扱うことで，複合語の意味を推定したい．しかしながら，複合語を正確に認定するのは大変むずかしいので，ここでは複合語からなる国名を認定したい．\n",
    "\n",
    "インターネット上から国名リストを各自で入手し，80のコーパス中に出現する複合語の国名に関して，スペースをアンダーバーに置換せよ．例えば，\"United States\"は\"United_States\"，\"Isle of Man\"は\"Isle_of_Man\"になるはずである"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original = ISO3166::Country.all.map{|country| country.name.sub(/\\s\\(.*?\\)\\s/, '')}.select{|name| name.match(/\\s/)}\n",
    "under_scored = original.map{|name| name.gsub(/\\s/, '_')}\n",
    "\n",
    "doc.each_with_index do |sentence, index|\n",
    "  p index if index % 50000 == 0\n",
    "  original.size.times do |i|\n",
    "    sentence.gsub!(original[i-1], under_scored[i-1])\n",
    "  end\n",
    "end\n",
    "\n",
    "output = open('../data/knock81.txt', 'w')\n",
    "doc.each do |sentence|\n",
    "  output.puts sentence\n",
    "end\n",
    "output.close\n",
    "\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 82. 文脈の抽出\n",
    "\n",
    "81で作成したコーパス中に出現するすべての単語ttに関して，単語ttと文脈語ccのペアをタブ区切り形式ですべて書き出せ．ただし，文脈語の定義は次の通りとする．\n",
    "\n",
    "ある単語ttの前後dd単語を文脈語ccとして抽出する（ただし，文脈語に単語ttそのものは含まない）\n",
    "単語ttを選ぶ度に，文脈幅ddは{1,2,3,4,5}{1,2,3,4,5}の範囲でランダムに決める．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def window\n",
    "  (Random.rand * 4.9999).to_i + 1\n",
    "end\n",
    "\n",
    "context_list = []\n",
    "\n",
    "doc.each_with_index do |words, index|\n",
    "  p index if index % 50000 == 0\n",
    "  words = words.split(/\\s|\\n/)\n",
    "  \n",
    "  words.size.times.each do |i|\n",
    "    w = window\n",
    "\n",
    "    start_from = [0, i - w].max\n",
    "    end_to = [words.size-1, i + w].min\n",
    "\n",
    "    contexts = words[start_from...i] + words[(i+1)..end_to]\n",
    "    contexts.each do |context|\n",
    "      context_list << [words[i], context]\n",
    "    end\n",
    "  end\n",
    "end\n",
    "\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 83. 単語／文脈の頻度の計測\n",
    "\n",
    "82の出力を利用し，以下の出現分布，および定数を求めよ．\n",
    "\n",
    "f(t,c)f(t,c): 単語ttと文脈語ccの共起回数\n",
    "f(t,∗)f(t,∗): 単語ttの出現回数\n",
    "f(∗,c)f(∗,c): 文脈語ccの出現回数\n",
    "NN: 単語と文脈語のペアの総出現回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: Marshal can't dump Hash object with default proc\n",
    "# count_t = Hash.new{|hash, key| hash[key] = 0}\n",
    "# count_c = Hash.new{|hash, key| hash[key] = 0}\n",
    "# count_t_c = Hash.new{|hash, key| hash[key] = Hash.new{|hash2, key2| hash2[key2] = 0}}\n",
    "\n",
    "count_t = Hash.new\n",
    "count_c = Hash.new\n",
    "count_t_c = Hash.new\n",
    "n = 0\n",
    "\n",
    "context_list.each_with_index do |(t, c), index|\n",
    "  p index if index % 10000000 == 0\n",
    "\n",
    "  # NOTE: alternative to default proc\n",
    "  count_t[t] = 0 unless count_t[t]\n",
    "  count_c[c] = 0 unless count_c[c]\n",
    "  count_t_c[t] = Hash.new unless count_t_c[t]\n",
    "  count_t_c[t][c] = 0 unless count_t_c[t][c]\n",
    "\n",
    "  count_t[t] += 1\n",
    "  count_c[c] += 1\n",
    "  count_t_c[t][c] += 1\n",
    "  n += 1\n",
    "end\n",
    "\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 84. 単語文脈行列の作成\n",
    "\n",
    "83の出力を利用し，単語文脈行列XXを作成せよ．ただし，行列XXの各要素XtcXtcは次のように定義する．\n",
    "\n",
    "f(t,c)≥10f(t,c)≥10ならば，Xtc=PPMI(t,c)=max{logN×f(t,c)f(t,∗)×f(∗,c),0}Xtc=PPMI(t,c)=max{log⁡N×f(t,c)f(t,∗)×f(∗,c),0}\n",
    "f(t,c)<10f(t,c)<10ならば，Xtc=0Xtc=0\n",
    "ここで，PPMI(t,c)PPMI(t,c)はPositive Pointwise Mutual Information（正の相互情報量）と呼ばれる統計量である．なお，行列XXの行数・列数は数百万オーダとなり，行列のすべての要素を主記憶上に載せることは無理なので注意すること．幸い，行列XXのほとんどの要素は00になるので，非00の要素だけを書き出せばよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = Hash.new\n",
    "i = 0\n",
    "\n",
    "token2id = Hash.new\n",
    "id2token = Hash.new\n",
    "\n",
    "count_t.keys.each_with_index do |token, id|\n",
    "  token2id[token] = id\n",
    "  id2token[id] = token\n",
    "end\n",
    "\n",
    "count_t_c.each do |t, hash|\n",
    "  p i if i % 10000000 == 0\n",
    "  hash.each do |c, val|\n",
    "    ppmi1 = Math.log((n*val) / (count_t[t]*count_c[c]))\n",
    "    f[token2id[t]] = Hash.new unless f[token2id[t]]\n",
    "    f[token2id[t]][token2id[c]] = [0, ppmi1].max if val >= 10\n",
    "    i += 1\n",
    "  end\n",
    "end\n",
    "\n",
    "f.reject! {|key, hash| hash.size == 0}\n",
    "data = [f, token2id]\n",
    "\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 85. 主成分分析による次元圧縮\n",
    "\n",
    "84で得られた単語文脈行列に対して，主成分分析を適用し，単語の意味ベクトルを300次元に圧縮せよ．\n",
    "\n",
    "保留"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 86. 単語ベクトルの表示\n",
    "\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"のベクトルを表示せよ．ただし，\"United States\"は内部的には\"United_States\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"United_States\"\n",
      "nil\n",
      "nil\n",
      "\"US\"\n",
      "nil\n",
      "nil\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: I have not done knock85 because of the limitation of Numo::NArray and NMatrix.\n",
    "#       So, this knock have not been completed yet.\n",
    "\n",
    "p 'United_States'\n",
    "p token2id['United_States']\n",
    "p f[token2id['United_States']]\n",
    "p 'US'\n",
    "p token2id['US']\n",
    "p f[token2id['US']]\n",
    "\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 87. 単語の類似度\n",
    "\n",
    "85で得た単語の意味ベクトルを読み込み，\"United States\"と\"U.S.\"のコサイン類似度を計算せよ．ただし，\"U.S.\"は内部的に\"U.S\"と表現されていることに注意せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":sim"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sim(v1, v2)\n",
    "  v1.transpose.dot(v2) / Math.sqrt(v1.transpose.dot(v1) * v2.transpose.dot(v2))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 88. 類似度の高い単語10件\n",
    "\n",
    "85で得た単語の意味ベクトルを読み込み，\"England\"とコサイン類似度が高い10語と，その類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":distance"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distance(mat, v1)\n",
    "  result = Hash.new\n",
    "  mat.shape[0].times do |i|\n",
    "    v2 = mat[i, true]\n",
    "    similarity = sim(v1, v2)\n",
    "    result[i] = similarity\n",
    "  end\n",
    "\n",
    "  result.sort{|(_, val1), (_, val2)| val2 <=> val1}[1..10]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 89. 加法構成性によるアナロジー\n",
    "\n",
    "85で得た単語の意味ベクトルを読み込み，vec(\"Spain\") - vec(\"Madrid\") + vec(\"Athens\")を計算し，そのベクトルと類似度の高い10語とその類似度を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":analogy"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analogy(mat, v1, v2, v3)\n",
    "  result = Hash.new\n",
    "  v4 = v1 - v2 + v3\n",
    "\n",
    "  mat.shape[0].times do |i|\n",
    "    v_ = mat[i, true]\n",
    "    similarity = sim(v4, v_)\n",
    "    result[i] = similarity\n",
    "  end\n",
    "\n",
    "  result.sort{|(_, val1), (_, val2)| val2 <=> val1}[1..10]\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.3.0",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
