{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'countries'\n",
    "require 'numo/narray'\n",
    "require 'nyaplot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":analogy"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sim(v1, v2)\n",
    "  v1.transpose.dot(v2) / Math.sqrt(v1.transpose.dot(v1) * v2.transpose.dot(v2))\n",
    "end\n",
    "\n",
    "def distance(mat, v1)\n",
    "  result = Hash.new\n",
    "  mat.shape[0].times do |i|\n",
    "    v2 = mat[i, true]\n",
    "    similarity = sim(v1, v2)\n",
    "    result[i] = similarity\n",
    "  end\n",
    "\n",
    "  result.sort{|(_, val1), (_, val2)| val2 <=> val1}[1..10]\n",
    "end\n",
    "\n",
    "def analogy(mat, v1, v2, v3)\n",
    "  result = Hash.new\n",
    "  v4 = v1 - v2 + v3\n",
    "\n",
    "  mat.shape[0].times do |i|\n",
    "    v_ = mat[i, true]\n",
    "    similarity = sim(v4, v_)\n",
    "    result[i] = similarity\n",
    "  end\n",
    "\n",
    "  result.sort{|(_, val1), (_, val2)| val2 <=> val1}[1..10]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 90. word2vecによる学習\n",
    "\n",
    "81で作成したコーパスに対してword2vecを適用し，単語ベクトルを学習せよ．さらに，学習した単語ベクトルの形式を変換し，86-89のプログラムを動かせ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Errno::ENOENT",
     "evalue": "No such file or directory @ rb_sysopen - ../data/knock81.dump",
     "output_type": "error",
     "traceback": [
      "\u001b[31mErrno::ENOENT\u001b[0m: No such file or directory @ rb_sysopen - ../data/knock81.dump",
      "\u001b[37m<main>:1:in `initialize'\u001b[0m",
      "\u001b[37m<main>:1:in `open'\u001b[0m",
      "\u001b[37m<main>:1:in `<main>'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/backend.rb:44:in `eval'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/backend.rb:44:in `eval'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/backend.rb:12:in `eval'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/kernel.rb:87:in `execute_request'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/kernel.rb:47:in `dispatch'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/kernel.rb:37:in `run'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/command.rb:70:in `run_kernel'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/command.rb:34:in `run'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/bin/iruby:5:in `<top (required)>'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/bin/iruby:23:in `load'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/bin/iruby:23:in `<main>'\u001b[0m"
     ]
    }
   ],
   "source": [
    "unless File.exists?('../data/knock90')\n",
    "  corpus = Marshal.load(open('../data/knock81.dump'))\n",
    "  output = open('../data/enwiki_corpus.txt', 'w')\n",
    "  corpus.each do |document|\n",
    "    output.puts document\n",
    "  end\n",
    "  output.close\n",
    "\n",
    "  puts 'NOTE: after running this script, please execute following command to create embedding model.'\n",
    "  puts 'word2vec  -train ../../data/enwiki_corpus.txt -output ../../data/knock90'\n",
    "  puts 'thanks !'\n",
    "\n",
    "else\n",
    "\n",
    "  token2id = Hash.new\n",
    "  id2token = Hash.new\n",
    "  mat = nil\n",
    "\n",
    "  File.foreach('../data/knock90').with_index do |line, index|\n",
    "    elements = line.chomp.split\n",
    "\n",
    "    if index == 0\n",
    "      i, j = elements.map(&:to_i)\n",
    "      mat = Numo::Float64.zeros(i, j)\n",
    "      next\n",
    "    end\n",
    "\n",
    "    id = index - 1\n",
    "    token = elements[0]\n",
    "    vector = elements[1..-1].map(&:to_f)\n",
    "\n",
    "    token2id[token] = id\n",
    "    id2token[id] = token\n",
    "\n",
    "    vector.each_with_index do |xy, d|\n",
    "      mat[id, d] = xy\n",
    "    end\n",
    "  end\n",
    "\n",
    "end\n",
    "\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_matrix = Numo::Float64.zeros(mat.size, mat[0].size)\n",
    "mat.each_with_index do |vector, i|\n",
    "  vector.each_with_index do |x, j|\n",
    "    word_matrix[i, j] = x\n",
    "  end\n",
    "end\n",
    "\n",
    "\n",
    "vec1 = word_matrix[token2id['United_States'], true]\n",
    "vec2 = word_matrix[token2id['US'], true]\n",
    "\n",
    "p 'United_States'\n",
    "p token2id['United_States']\n",
    "p vec1\n",
    "p 'US'\n",
    "p token2id['US']\n",
    "p vec2\n",
    "\n",
    "\n",
    "\n",
    "mat.each_with_index do |vector, i|\n",
    "  vector.each_with_index do |x, j|\n",
    "    word_matrix[i, j] = x\n",
    "  end\n",
    "end\n",
    "\n",
    "vec1 = word_matrix[token2id['United_States'], true]\n",
    "vec2 = word_matrix[token2id['US'], true]\n",
    "\n",
    "p sim(vec1, vec2)\n",
    "\n",
    "vec1 = word_matrix[token2id['England'], true]\n",
    "distance(word_matrix, vec1).map{|id, sim| [id2token[id], sim]}.each do |token, sim|\n",
    "  puts \"#{token}\\t#{sim}\"\n",
    "end\n",
    "\n",
    "\n",
    "vec1 = word_matrix[token2id['Spain'], true]\n",
    "vec2 = word_matrix[token2id['Madrid'], true]\n",
    "vec3 = word_matrix[token2id['Athens'], true]\n",
    "\n",
    "analogy(word_matrix, vec1, vec2, vec3).map{|id, sim| [id2token[id], sim]}.each do |token, sim|\n",
    "  puts \"#{token}\\t#{sim}\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意\n",
    "\n",
    "81でつくったdocをテキストファイルに出力したやつ(../data/enwiki_for_knock81.txt)を使う\n",
    "\n",
    "```ruby\n",
    "doc.each do |sentence|\n",
    "  puts sentence\n",
    "end\n",
    "```\n",
    "\n",
    "しただけ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 91. アナロジーデータの準備\n",
    "\n",
    "単語アナロジーの評価データをダウンロードせよ．このデータ中で\": \"で始まる行はセクション名を表す．例えば，\": capital-common-countries\"という行は，\"capital-common-countries\"というセクションの開始を表している．ダウンロードした評価データの中で，\"family\"というセクションに含まれる評価事例を抜き出してファイルに保存せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problems = []\n",
    "writable = false\n",
    "\n",
    "File.foreach('../../data/questions-words.txt').map(&:chomp).each do |line|\n",
    "  if line == \": family\"\n",
    "    writable = true\n",
    "    next\n",
    "  end\n",
    "\n",
    "  if line =~ /^\\:/ && writable\n",
    "    break\n",
    "  end\n",
    "\n",
    "  problems << line.split if writable\n",
    "end\n",
    "\n",
    "Marshal.dump(problems, open('../../data/knock91.dump', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 92. アナロジーデータへの適用\n",
    "\n",
    "91で作成した評価データの各事例に対して，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．求めた単語と類似度は，各事例の末尾に追記せよ．このプログラムを85で作成した単語ベクトル，90で作成した単語ベクトルに対して適用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply(word_matrix, token2id, id2token, problems)\n",
    "  results = []\n",
    "\n",
    "  problems.each do |problem|\n",
    "    next unless token2id[problem[0]] && token2id[problem[1]] && token2id[problem[2]] && token2id[problem[3]]\n",
    "    vec1 = word_matrix[token2id[problem[0]], true]\n",
    "    vec2 = word_matrix[token2id[problem[1]], true]\n",
    "    vec3 = word_matrix[token2id[problem[2]], true]\n",
    "    vec4 = word_matrix[token2id[problem[3]], true]\n",
    "\n",
    "    most_similar_id = analogy(word_matrix, vec1, vec2, vec3)[0][0]\n",
    "    vec4_ = word_matrix[most_similar_id, true]\n",
    "    similarity = sim(vec4, vec4_)\n",
    "\n",
    "    puts \"#{problem[3]}\\t#{id2token[most_similar_id]}\\t#{similarity}\"\n",
    "    results << [problem[3], id2token[most_similar_id], similarity]\n",
    "  end\n",
    "\n",
    "  results\n",
    "end\n",
    "\n",
    "\n",
    "puts \"applying start... (it takes few minitutes)\"\n",
    "results = apply(word_matrix, token2id, id2token, problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 93. アナロジータスクの正解率の計算\n",
    "\n",
    "92で作ったデータを用い，各モデルのアナロジータスクの正解率を求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_sims = results.map{|a, b, c| c}\n",
    "puts \"score: #{result_sims.select{|c| c == 1}.size.to_f / result_sims.size}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 94. WordSimilarity-353での類似度計算\n",
    "\n",
    "The WordSimilarity-353 Test Collectionの評価データを入力とし，1列目と2列目の単語の類似度を計算し，各行の末尾に類似度の値を追加するプログラムを作成せよ．このプログラムを85で作成した単語ベクトル，90で作成した単語ベクトルに対して適用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "File.foreach('../../data/combined.tsv') do |line|\n",
    "  word1, word2, score = line.split\n",
    "  score = score.to_f\n",
    "  \n",
    "  next unless token2id[word1] && token2id[word2]\n",
    "  similarity = sim(word_matrix[token2id[word1], true], word_matrix[token2id[word2], true])\n",
    "  puts \"#{word1}\\t#{word2}\\t#{similarity}\\t#{score}\"\n",
    "  results << [word1, word2, similarity, score]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95. WordSimilarity-353での評価\n",
    "\n",
    "94で作ったデータを用い，各モデルが出力する類似度のランキングと，人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ．\n",
    "\n",
    "保留"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 96. 国名に関するベクトルの抽出\n",
    "\n",
    "word2vecの学習結果から，国名に関するベクトルのみを抜き出せ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries.each do |country|\n",
    "  if token2id[country]\n",
    "    p country\n",
    "    p word_matrix[token2id[country], true]\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 97. k-meansクラスタリング\n",
    "\n",
    "96の単語ベクトルに対して，k-meansクラスタリングをクラスタ数k=5k=5として実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: we can get data more simply (now, I do not use this procedure because k-means needs to be fixed)\n",
    "# data = countries.map {|country| token2id[country] ? mat[token2id[country]] : nil }.select {|arr| arr}\n",
    "# mat = Numo::Float64[*data]\n",
    "\n",
    "vectors = []\n",
    "labels = []\n",
    "num_class = []\n",
    "\n",
    "countries.each do |country|\n",
    "  if token2id[country]\n",
    "    labels << country\n",
    "    vectors << word_matrix[token2id[country], true]\n",
    "    num_class << 0\n",
    "  end\n",
    "end\n",
    "\n",
    "# k-means\n",
    "#\n",
    "# parameter\n",
    "k = 5\n",
    "\n",
    "# initialize\n",
    "centroids = vectors.shuffle.first(k)\n",
    "\n",
    "10.times do |num_iter|\n",
    "  puts \"iter: #{num_iter}\"\n",
    "\n",
    "  # assign phase\n",
    "  vectors.each_with_index do |vector, i|\n",
    "    similarities = centroids.map do |centroid|\n",
    "      sim(vector, centroid)\n",
    "    end\n",
    "\n",
    "    max_sim = similarities.max\n",
    "    argmax = similarities.index(max_sim)\n",
    "    num_class[i] = argmax\n",
    "  end\n",
    "\n",
    "  # maximization\n",
    "  centroids.size.times do |t|\n",
    "    new_centroid = Numo::Float64.zeros(centroids[0].shape)\n",
    "\n",
    "    num_points = 0\n",
    "    vectors.each_with_index do |vector, i|\n",
    "      if num_class[i] == t\n",
    "        new_centroid += vector \n",
    "        num_points += 1\n",
    "      end\n",
    "    end\n",
    "\n",
    "    centroids[t] = new_centroid / num_points\n",
    "  end\n",
    "end\n",
    "\n",
    "puts\n",
    "\n",
    "centroids.size.times do |i|\n",
    "  puts \"cluster_#{i}\"\n",
    "\n",
    "  cluster = []\n",
    "  vectors.size.times do |t|\n",
    "    cluster << labels[t] if num_class[t] == i\n",
    "  end\n",
    "\n",
    "  puts cluster.sort.join(' ')\n",
    "  puts\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 98. Ward法によるクラスタリング\n",
    "96の単語ベクトルに対して，Ward法による階層型クラスタリングを実行せよ．さらに，クラスタリング結果をデンドログラムとして可視化せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 99. t-SNEによる可視化\n",
    "96の単語ベクトルに対して，ベクトル空間をt-SNEで可視化せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Errno::ENOENT",
     "evalue": "No such file or directory @ rb_sysopen - ../data/knock90.dump",
     "output_type": "error",
     "traceback": [
      "\u001b[31mErrno::ENOENT\u001b[0m: No such file or directory @ rb_sysopen - ../data/knock90.dump",
      "\u001b[37m<main>:5:in `initialize'\u001b[0m",
      "\u001b[37m<main>:5:in `open'\u001b[0m",
      "\u001b[37m<main>:5:in `<main>'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/backend.rb:44:in `eval'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/backend.rb:44:in `eval'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/backend.rb:12:in `eval'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/kernel.rb:87:in `execute_request'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/kernel.rb:47:in `dispatch'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/kernel.rb:37:in `run'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/command.rb:70:in `run_kernel'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/lib/iruby/command.rb:34:in `run'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/gems/iruby-0.2.9/bin/iruby:5:in `<top (required)>'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/bin/iruby:23:in `load'\u001b[0m",
      "\u001b[37m/Users/himkt/Projects/nlp-100knock/ruby-notebook/bundle/ruby/2.3.0/bin/iruby:23:in `<main>'\u001b[0m"
     ]
    }
   ],
   "source": [
    "countries = ISO3166::Country.all.map{|obj| obj.name.gsub(/\\s/, '_')}\n",
    "token2id, _, mat = Marshal.load(open('../data/knock90.dump', 'rb'))\n",
    "\n",
    "data = countries.map {|country| token2id[country] ? mat[token2id[country]] : nil }.select {|arr| arr}\n",
    "names = countries.map {|country| token2id[country] ? country : nil}.select {|arr| arr}\n",
    "mat = Numo::Float64[*data]\n",
    "\n",
    "\n",
    "# TODO: random initialize should be done with normal distribution\n",
    "tsne_mat = Numo::Float64.new(mat.shape[0], 2).rand\n",
    "\n",
    "# perp = 0.1\n",
    "num_iter = 10\n",
    "eta = 0.01\n",
    "# momentum = alpha\n",
    "\n",
    "# compute p_{i, j}\n",
    "p = Numo::Float64.zeros(mat.shape[0], mat.shape[0])\n",
    "q = Numo::Float64.zeros(mat.shape[0], mat.shape[0])\n",
    "\n",
    "# I compute denominator here because denominator of p_{i, j} (appeared in Equation (3)) is constant when X are given.\n",
    "omega = 1 # TODO: need to compute a variance\n",
    "denominator = 0\n",
    "tsne_mat.shape[0].size.times do |k|\n",
    "  tsne_mat.shape[0].size.times do |l|\n",
    "    next if k == l\n",
    "    elem = mat[k, true] - mat[l, true]\n",
    "    denominator += Math.exp(- elem.transpose.dot(elem) / omega)\n",
    "  end\n",
    "end\n",
    "\n",
    "\n",
    "tsne_mat.shape[0].times do |i|\n",
    "  tsne_mat.shape[0].times do |j|\n",
    "    elem1 = mat[i, true] - mat[j, true]\n",
    "    numerator = Math.exp(- elem1.transpose.dot(elem1) / omega)\n",
    "    p_j_i = numerator / denominator\n",
    "    elem2 = mat[j, true] - mat[j, true]\n",
    "    numerator = Math.exp(- elem2.transpose.dot(elem2) / omega)\n",
    "    p_i_j = numerator / denominator\n",
    "    p[i, j] = p_i_j + p_j_i\n",
    "  end\n",
    "end\n",
    "\n",
    "\n",
    "num_iter.times do |t|\n",
    "  denominator = 0\n",
    "  tsne_mat.shape[0].size.times do |k|\n",
    "    tsne_mat.shape[0].size.times do |l|\n",
    "      next if k == l\n",
    "      elem = tsne_mat[k, true] - tsne_mat[l, true]\n",
    "      denominator += 1 + elem.transpose.dot(elem)\n",
    "    end\n",
    "  end\n",
    "\n",
    "  tsne_mat.shape[0].size.times do |i|\n",
    "    tsne_mat.shape[0].size.times do |j|\n",
    "      # compute low-dimensional affinities q_{i, j} using Equation 4\n",
    "      elem1 = tsne_mat[i, true] - tsne_mat[j, true]\n",
    "      numerator = 1 + elem1.transpose.dot(elem1)\n",
    "\n",
    "      # NOTE: numerator^{-1} / denominator^{-1}\n",
    "      q[i, j] = denominator / numerator\n",
    "    end\n",
    "  end\n",
    "\n",
    "  # compute gradient (using Equation 5)\n",
    "  gradient = Numo::Float64.zeros(tsne_mat.shape)\n",
    "  tsne_mat.shape[0].times do |i|\n",
    "    # numerator = 0\n",
    "    tsne_mat.shape[0].times do |j|\n",
    "      elem1 = p[i, j] - q[i, j] # scalar\n",
    "      elem2 = (tsne_mat[i, true] - tsne_mat[j, true]) # vector\n",
    "      elem3 = 1 / (1 + elem2.transpose.dot(elem2)) # scalar\n",
    "\n",
    "      dcdy_i = 4 * elem1 * elem2 / elem3\n",
    "\n",
    "      # TODO: use for loop\n",
    "      gradient[i, 0] = dcdy_i[0]\n",
    "      gradient[i, 1] = dcdy_i[1]\n",
    "    end\n",
    "  end\n",
    "\n",
    "  # update\n",
    "  tsne_mat = tsne_mat + eta * gradient\n",
    "end\n",
    "\n",
    "\n",
    "x = tsne_mat[true, 0]\n",
    "y = tsne_mat[true, 1]\n",
    "\n",
    "plt = Nyaplot::Plot.new\n",
    "plt.add(:scatter, x, y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.3.0",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
